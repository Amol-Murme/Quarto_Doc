editable_Headings_h1: EXECUTIVE SUMMARY
editable_Headings_h1_p: This document covers the model development process for **XGBoost_2_AutoML_20210218_195405
  **model. The model is a classification model that uses **XGBoost **with input data
  consisting of **20000 **observations and **70 **features. The model achieves Auto
  of **75.84% **on validation dataset and **74.98% **on Out-of-Sample (OOS) test dataset.
editable_Headings_h2: MODEL PERFORMANCE SUMMARY
editable_Headings_h3: DATASET
editable_Headings_h3_p: Following dataset were used for model training, tuning and
  OOS performance estimation
editable_Headings_h4: EDA
editable_Headings_h4_p: Following is a summary of input data. Refer Annexure-1 for
  detailed EDA.
editable_Headings_h5: Methodology Overview
editable_Headings_h5_1: Data Preparation
editable_Headings_h5_1_p: "The dataset is randomly split into train, validation and\
  \ holdout test datasets. Train data is used for model fitting. Validation dataset\
  \ is used for model tuning i.e.\_finding the optimal combination of hyperparameters\
  \ that provide the best fit on a given dataset. Holdout test dataset is used to\
  \ arrive at an unbiased estimate of OOS performance of the model."
editable_Headings_h5_2: Feature Transformation
editable_Headings_h5_2_p: Typically all features are converted into numeric features.
  This is a mandatory transformation for many algorithms such as XGBoost.
editable_Headings_h5_3: Model Tuning
editable_Headings_h5_3_p: Various models are fitted to the train dataset with multiple
  combination of hyperparameters (HP). These HP typically control model capacity (large
  capacity models will provide better fit on train data but may fail to generalize
  to OOS dataset), model complexity (typically models with larger capacity are also
  more complex) and model generalization (to prevent overfitting to train data).
editable_Headings_h5_4: Model Performance Evaluation
editable_Headings_h5_4_p: Performance of trained models is compared on validation
  dataset using different statistics. Final HP combination and the resultant final
  model is selected on basis of performance on the validation dataset.
editable_Headings_h5_5: Model Stability
editable_Headings_h5_5_p: Model stability is checked by detecting drift/shift in features
  between train, validation and test dataset. This is done by computing Stability
  Index at model and individual feature level to identify if model is stable or not.
editable_Headings_h5_p: XGBoost is a machine learning technique for regression and
  classification problems, which produces a prediction model in the form of an ensemble
  of weak prediction models, typically decision trees. It builds the model in a stage-wise
  fashion by optimization of a loss function.
editable_Headings_h5_p1: 'Following is a summary of steps performed to train the model:'
editable_Headings_h6: Model Details
editable_Headings_h6_1: Model Hyperparameters
editable_Headings_h6_1_p: 'Following is a summary of key model hyperparameters:'
editable_Headings_h6_2: Important Features
editable_Headings_h6_2_p: 'Following is a list of important features for the model:'
editable_Headings_h6_2_p1: 'Partial Dependence Plots for Top-2 variables are shown
  below:'
editable_Headings_h6_3: Model Performance
editable_Headings_h6_3_p: 'Following are the model performance statistics on validation
  and OOS test dataset:'
editable_Headings_h6_4: Model Stability Statistics
editable_Headings_h6_4_p: 'Following are model stability statistics:'
editable_Headings_h6_5: Model Scoring History Reloaded..
editable_Headings_h6_5_p: Following is a summary of change in model performance statistics
  with increase in number of trees.
editable_Headings_h6_p: Detailed Information regarding model.
